{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1812b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d1c6c",
   "metadata": {},
   "source": [
    "Problem 4 on midterm\n",
    "Consider a finite horizon LQR problem over a horizon of H = 20 with dynamics:\n",
    "x1(t+1) = x1(t) - 0.25x2(t) + 0.4u1(t) \n",
    "x2(t+1) = x2(t) + 0.1u1(t) + 0.1u2(t)\n",
    "\n",
    "and cost:\n",
    "\n",
    "c(x(t), u(t)) = 2x1^2(t) + 5x2^2(t) + 10u1^2(t) + 8u2^2(t)\n",
    "\n",
    "x(t), u(t) are in R2 (2 dimensions)\n",
    "\n",
    "Part 1 asked us to find the transition matrices A, B and cost matrices Q, and R. Note, there is no noise in these dynamics.\n",
    "\n",
    "Those matrices can be found by simply looking at A, B, Q, R as identity matrices, then adjusting those identity matrices to match the dynamics and cost equations. \n",
    "\n",
    "They are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc31af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A\n",
      " [[ 1.   -0.25]\n",
      " [ 0.    1.  ]] \n",
      "\n",
      "Transition matrix B\n",
      " [[0.4 0. ]\n",
      " [0.1 0.1]] \n",
      "\n",
      "Cost matrix Q\n",
      " [[2 0]\n",
      " [0 5]] \n",
      "\n",
      "Cost matrix R\n",
      " [[10  0]\n",
      " [ 0  8]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, -0.25], [0, 1]])\n",
    "B = np.array([[0.4, 0], [0.1, 0.1]])\n",
    "\n",
    "Q = np.array([[2, 0], [0, 5]])\n",
    "R = np.array([[10, 0], [0, 8]])\n",
    "print(\"Transition matrix A\\n\", A, \"\\n\")\n",
    "print(\"Transition matrix B\\n\", B, \"\\n\")\n",
    "print(\"Cost matrix Q\\n\", Q, \"\\n\")\n",
    "print(\"Cost matrix R\\n\", R, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9880d",
   "metadata": {},
   "source": [
    "Part 2 asks to compute the matrix P(t) based on the Riccati equation for t = 20, 19, and 18\n",
    "Note that the Ph = Q. We have computed Q above.\n",
    "\n",
    "Furthermore, Pt = A.T @ Pt+1 @ A + Q - A.T @ Pt + 1 @ B @ np.linalg.inv(B.T @ Pt+1 @ B + R) @ B.T @ Pt+1 @ A \n",
    "With this equation, we can compute Pt for t = 20, 19, and 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d645423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P20 is\n",
      " [[2 0]\n",
      " [0 5]] \n",
      "\n",
      "P19 is \n",
      " [[ 3.93828166 -0.52290479]\n",
      " [-0.52290479 10.08544372]] \n",
      "\n",
      "P18 is \n",
      " [[ 5.72077621 -1.55911559]\n",
      " [-1.55911559 15.44858428]]\n"
     ]
    }
   ],
   "source": [
    "P20 = Q\n",
    "Pt_plus_1 = Q\n",
    "print(\"P20 is\\n\", Q, \"\\n\")\n",
    "\n",
    "P19 = A.T @ Pt_plus_1 @ A + Q - A.T @ Pt_plus_1 @ B @ np.linalg.inv(B.T @ Pt_plus_1 @ B + R) @ B.T @ Pt_plus_1 \\\n",
    "@ A\n",
    "\n",
    "print(\"P19 is \\n\", P19, \"\\n\")\n",
    "Pt_plus_1 = P19\n",
    "\n",
    "P18 = A.T @ Pt_plus_1 @ A + Q - A.T @ Pt_plus_1 @ B @ np.linalg.inv(B.T @ Pt_plus_1 @ B + R) @ B.T @ Pt_plus_1 \\\n",
    "@ A\n",
    "\n",
    "print(\"P18 is \\n\", P18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a62f1",
   "metadata": {},
   "source": [
    "Part 3 asks us to compute the optial control gain K*(t) (note * is for optimal, not multiply) for t = 19, 18\n",
    "\n",
    "We can compute K*(t), the optimal control gain, by:\n",
    "K_opt_t = np.linalg.inv(B.T @ Pt_plus_1 @ B + R) @ B.T @ Pt_plus_1 @ A\n",
    "\n",
    "The computations for optimal control gain at t = 19, 18 are done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53a364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controller gain at t = 19\n",
      " [[ 0.07714792  0.02863098]\n",
      " [-0.00047918  0.06193397]] \n",
      "\n",
      "controller gain at t = 18\n",
      " [[ 0.14254183  0.03822343]\n",
      " [-0.00786152  0.12573507]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K_opt_19:\n",
    "Pt_plus_1 = P20\n",
    "K_opt_19 = np.linalg.inv(B.T @ Pt_plus_1 @ B + R) @ B.T @ Pt_plus_1 @ A\n",
    "print(\"controller gain at t = 19\\n\", K_opt_19, \"\\n\")\n",
    "\n",
    "# K_opt_18:\n",
    "Pt_plus_1 = P19\n",
    "K_opt_18 = np.linalg.inv(B.T @ Pt_plus_1 @ B + R) @ B.T @ Pt_plus_1 @ A\n",
    "print(\"controller gain at t = 18\\n\", K_opt_18, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f692af1",
   "metadata": {},
   "source": [
    "Part 4:\n",
    "Write the optimal policy as a function of state for t = 19, t = 18\n",
    "\n",
    "The optimal polcy is a linear controller defined by:\n",
    "pi_opt = -K_opt_t @ xt, where xt is a state vector, K_opt_t is the controller gain for a particular time step (was computed above). The output pi_opt will be a vector for the optimal action.\n",
    "\n",
    "Code to compute this is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32003de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_opt_19 = -K_opt_19 @ xt_19 # note, we would actually need state values to compute this\n",
    "pi_opt_18 = -K_opt_18 @ xt_18 # note, we would actually need state values to compute this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5d38a",
   "metadata": {},
   "source": [
    "Part 5: Write the optimal value function as a function of state for t = 20, 19, and 18.\n",
    "\n",
    "Optimal value function is defined as:\n",
    "v_opt_t = x.T @ Pt @ x + noise_var * (sum of traces of P from t' = t + 1 to H).\n",
    "However, we do not have noise in this problem, therefore, v_opt_t = x.T @ Pt @ x\n",
    "The code for this is written below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, we would not be able to compute these until we have an input state x\n",
    "v_opt_20 = x.T @ P20 @ x\n",
    "v_opt_19 = x.T @ P19 @ x\n",
    "v_opt_18 = x.T @ P18 @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8a7bc",
   "metadata": {},
   "source": [
    "Question 1-2: We have a Markov Decision process with states = {action, horror, comedy} and actions = {movieA, movieB, movieC, movieD}. The initial state distribution mu_0 is uniform across states. The transition function p(s' | s, a) has been computed. \n",
    "\n",
    "We are given a policy pi(a | s). \n",
    "\n",
    "Specifically:\n",
    "If the user asks for action, the action is movieA = 0.45, movieB = 0.45, movieD = 0.1\n",
    "If the user asks for horror, the action is movieA = movieB = movieC = movieD = 0.25\n",
    "If the user asks for comedy, the action is movie B = 0.3, movieC = 0.7\n",
    "\n",
    "We can compute R_pi(s) as a 3 dimension vector. Each component is computed by summing over actions\n",
    "We can compute P_pi(s,s') as a 3 by 3 matrix. Each component represents the probability of transitioning from \n",
    "state s to state s' given the transition function and policy. The components of R_pi and P_pi are hand computed. The code below is inserting the hand computed values in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35d1e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_pi vector:\n",
      " [[1. ]\n",
      " [0.8]\n",
      " [0.5]] \n",
      "\n",
      "P_pi\n",
      " [[0.    0.5   0.5  ]\n",
      " [0.45  0.1   0.45 ]\n",
      " [0.375 0.375 0.25 ]] \n",
      "\n",
      "Value for state comedy : [37.28921636]\n",
      "Value for state action : [37.15922497]\n",
      "Value for state horror : [36.90040025]\n"
     ]
    }
   ],
   "source": [
    "R_pi = np.zeros((3,1))\n",
    "R_pi[0] = 1\n",
    "R_pi[1] = 0.8\n",
    "R_pi[2] = 0.5\n",
    "\n",
    "P_pi = np.array([[0, 0.5, 0.5],\n",
    "                [.45, .1, .45],\n",
    "                [.375, .375, .25]])\n",
    "\n",
    "print(\"R_pi vector:\\n\", R_pi, \"\\n\")\n",
    "print(\"P_pi\\n\", P_pi, \"\\n\")\n",
    "I = np.eye(P_pi.shape[0])\n",
    "gamma = 0.98\n",
    "V_pi = np.linalg.inv(I - gamma * P_pi) @ R_pi\n",
    "states = {0: 'comedy', 1: 'action', 2: 'horror'}\n",
    "for i in range(V_pi.shape[0]):\n",
    "    print(\"Value for state\", states[i], \":\", V_pi[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ece637]",
   "language": "python",
   "name": "conda-env-ece637-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
