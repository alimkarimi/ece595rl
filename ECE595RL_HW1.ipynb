{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84bd0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6636e8",
   "metadata": {},
   "source": [
    "Problem 2-5:\n",
    "What will be the state distribution after two transitions, i.e., Î¼2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3ab2cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of being in state 0 after 2 state transitions is 0.0\n",
      "probability of being in state 1 after 2 state transitions is 0.0\n",
      "probability of being in state 2 after 2 state transitions is 0.1\n",
      "probability of being in state 3 after 2 state transitions is 0.0\n",
      "probability of being in state 4 after 2 state transitions is 0.0\n",
      "probability of being in state 5 after 2 state transitions is 0.0\n",
      "probability of being in state 6 after 2 state transitions is 0.375\n",
      "probability of being in state 7 after 2 state transitions is 0.125\n",
      "probability of being in state 8 after 2 state transitions is 0.0\n",
      "probability of being in state 9 after 2 state transitions is 0.0\n",
      "probability of being in state 10 after 2 state transitions is 0.4\n",
      "probability of being in state 11 after 2 state transitions is 0.0\n"
     ]
    }
   ],
   "source": [
    "              #1   2   3. 4. 5. 6   7    8     9  10   11 12\n",
    "P = np.array([[0, 0.1, 0, 0, 0, 0, 0.5,  0,   0,  0.4, 0, 0],\n",
    "              [0, 0,   1, 0, 0, 0, 0,    0,   0,  0,   0, 0],\n",
    "              [0, 0,   0, 1, 0, 0, 0,    0,   0,  0,   0, 0],\n",
    "              [0, 0,   0, 0, 1, 0, 0,    0,   0,  0,   0, 0],\n",
    "              [0, 0.6, 0, 0, 0.4,0,0,    0,   0,  0,   0, 0],\n",
    "              [0, 0,   0, 0, 1, 0, 0,    0,   0,  0,   0, 0],\n",
    "              [0, 0,   0, 0, 0, 0, 0.75, 0.25,0,  0,   0, 0],\n",
    "              [0, 0,   0, 0, 0, 0, 0,    0,   1,  0,   0, 0],\n",
    "              [0, 0,   0, 0, 0, 0, 0.2,  0.8, 0,  0,   0, 0],\n",
    "              [0, 0,   0, 0, 0, 0, 0,    0,   0,  0,   1, 0],\n",
    "              [0, 0,   0, 0, 0, 0, 0,    0,   0,  0.9, 0, 0.1],\n",
    "              [0, 0,   0, 0, 0, 0, 0,    0,   0,  0,   0, 1]])\n",
    "\n",
    "\n",
    "\n",
    "m0 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "def markovianCheck(P):\n",
    "    \"\"\"\n",
    "    Input transition matrix, output \"error\" string if \n",
    "    it is not a valid Markovian matrix.\n",
    "    \"\"\"\n",
    "    for i in range(P.shape[0]):\n",
    "        temp_sum = np.sum(P[i])\n",
    "        if temp_sum !=1:\n",
    "            print('error on row', i)\n",
    "            \n",
    "markovianCheck(P) #no error is good.\n",
    "\n",
    "def computeStateDistribution(P, m0, num_state):\n",
    "    \"\"\"\n",
    "    Compute the state distributino given a transition matrix P and \n",
    "    the desired state index given by num_state and initial state distribution\n",
    "    m0\n",
    "    \"\"\"\n",
    "    for i in range(num_state - 1):\n",
    "        P = P @ P # multiply P by P by the number of time steps we want to get our state distribution.\n",
    "        # for example, if we want to know the state distrubtion at t=2 given m0, we need to do m0 @ P @ P. \n",
    "    state_dist = m0 @ P\n",
    "    assert(np.sum(state_dist) == 1)\n",
    "    return m0 @ P\n",
    "\n",
    "m2 = computeStateDistribution(P, m0, 2)\n",
    "\n",
    "for i in range(m2.shape[1]):\n",
    "    print('probability of being in state', i, 'after 2 state transitions is', m2[0,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055b3e7",
   "metadata": {},
   "source": [
    "Problem 5-1a: Compute the analytical solution (matrix inversion) for policy evaluation. Report the value function\n",
    "at all states.\n",
    "Note: The discount factor is 0.95.\n",
    "Agent gets reward of 0 at all states except lightning bolt (-1) and tresaure chest (+1)\n",
    "On a 4 x 4 grid, the treasure chest is at row 1, col 4 and the lightning bolt is at row 2, col 2 (1-indexing).\n",
    "\n",
    "Analytical solution for value function is solving for v_pi in v_pi = R_pi + gamma * P_pi * v_pi.\n",
    "R_pi is the reward function.\n",
    "P_pi is the transition matrix.\n",
    "v_pi is the value function. \n",
    "\n",
    "Note, after re-arranging, we can solve for v_pi by v_pi = inverse(I - gamma*P_pi) * R_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7e0cceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function at state 0 [11.23185702]\n",
      "value function at state 1 [12.06229028]\n",
      "value function at state 2 [14.74404741]\n",
      "value function at state 3 [20.]\n",
      "value function at state 4 [8.93749904]\n",
      "value function at state 5 [-20.]\n",
      "value function at state 6 [15.50556825]\n",
      "value function at state 7 [18.59902354]\n",
      "value function at state 8 [8.27880624]\n",
      "value function at state 9 [6.34578879]\n",
      "value function at state 10 [15.84138082]\n",
      "value function at state 11 [17.45379858]\n",
      "value function at state 12 [7.72857881]\n",
      "value function at state 13 [6.51005875]\n",
      "value function at state 14 [14.93682128]\n",
      "value function at state 15 [16.35739377]\n"
     ]
    }
   ],
   "source": [
    "grid = np.zeros((4,4))\n",
    "\n",
    "\n",
    "grid_actions = np.array([['r', 'r', 'd', 'u'], \n",
    "                         ['u', 'u', 'r', 'u'],\n",
    "                        ['u', 'l', 'r', 'u'], \n",
    "                         ['u', 'u', 'r', 'u']])\n",
    "\n",
    "grid_actions_test = np.array([['l', 'l', 'l', 'l'], \n",
    "                         ['l', 'l', 'l', 'l'],\n",
    "                        ['l', 'l', 'l', 'l'], \n",
    "                         ['l', 'l', 'l', 'l']])\n",
    "\n",
    "\n",
    "def computeTransitionFunction(grid_actions):\n",
    "    P = np.zeros((16,16))\n",
    "    for row in range(grid_actions.shape[0]):\n",
    "        for col in range(grid_actions.shape[1]):\n",
    "            policy = grid_actions[row,col] #up, down, left, right will be the policy. mapping from state to action\n",
    "            #check if policy has a boundary or unreachable state:\n",
    "            current_state = row * 4 + col\n",
    "            prob_stay = 0.0\n",
    "            ### POLICY TO THE RIGHT ###\n",
    "            if policy == 'r':\n",
    "                temp_col = col + 1\n",
    "                if (temp_col <= 3) and (temp_col >= 0): #valid\n",
    "                    next_state = (row, col + 1)\n",
    "                    next_state = row * 4 + col + 1\n",
    "                    P[current_state, next_state] = 0.85\n",
    "                if (temp_col > 3) or (temp_col < 0): # invalid move\n",
    "                    P[current_state, current_state] = 0.85\n",
    "                # check if other moves are valid:\n",
    "                # start with up:\n",
    "                if (row - 1 >= 0) and (row - 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = (row - 1) * 4 + col\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "                # check down:\n",
    "                if (row + 1 >= 0) and (row + 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = (row + 1) * 4 + col\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "                # check left\n",
    "                if (col - 1 >= 0) and (col - 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = row * 4 + col - 1\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "\n",
    "\n",
    "            ### POLICY TO THE LEFT ###\n",
    "            if policy == 'l':\n",
    "                temp_col = col - 1\n",
    "                if (temp_col <= 3) and (temp_col >= 0): #valid\n",
    "                    next_state = (row, col -1)\n",
    "                    next_state = row * 4 + col - 1\n",
    "                    P[current_state, next_state] = 0.85\n",
    "                if (temp_col > 3) or (temp_col < 0): # invalid move\n",
    "                    P[current_state, current_state] = 0.85\n",
    "                #check if other moves are valid:\n",
    "                # start with up:\n",
    "                if (row - 1 >= 0) and (row - 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = (row - 1) * 4 + col\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "                # check down:\n",
    "                if (row + 1 >= 0) and (row + 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = (row + 1) * 4 + col\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "                # check right:\n",
    "                if (col +1 >= 0) and (col +1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = row * 4 + col + 1\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if policy == 'u':\n",
    "                temp_row = row -1\n",
    "                if (temp_row <= 3) and (temp_row >= 0): #valid\n",
    "                    next_state = (row - 1) * 4 + col\n",
    "                    P[current_state, next_state] = 0.85\n",
    "                if (temp_row > 3) or (temp_row < 0): # invalid move\n",
    "                    prob_stay = prob_stay + 0.05 #hold this value. We need to know the total invalid moves\n",
    "                    P[current_state, current_state] = 0.85\n",
    "                # check if other moves are valid:\n",
    "                # check down:\n",
    "\n",
    "                if (row + 1 >= 0) and (row + 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = (row + 1) * 4 + col\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "                # check right:\n",
    "\n",
    "                if (col +1 >= 0) and (col +1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = row * 4 + col + 1\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "                # check left\n",
    "\n",
    "                if (col - 1 >= 0) and (col - 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = row * 4 + col - 1\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else: \n",
    "                        P[current_state, current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "            if policy == 'd':\n",
    "                temp_row = row + 1\n",
    "                if (temp_row <= 3) and (temp_row >= 0): #valid\n",
    "                    next_state = (row + 1) * 4 + col\n",
    "                    P[current_state, next_state] = 0.85\n",
    "                if (temp_row > 3) or (temp_row < 0): # invalid move\n",
    "                    P[current_state, current_state] = 0.85\n",
    "                # check if other moves are valid:\\\n",
    "                # check right:\n",
    "\n",
    "                if (col +1 >= 0) and (col +1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = row * 4 + col + 1\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else:\n",
    "                        P[current_state,current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "                # check left\n",
    "\n",
    "                if (col - 1 >= 0) and (col - 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = row * 4 + col - 1\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else:\n",
    "                        P[current_state,current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "                # check up:\n",
    "                if (row - 1 >= 0) and (row - 1 <=3):\n",
    "                    # this transition has a 0.05 probability\n",
    "                    next_state = (row - 1) * 4 + col\n",
    "                    if (next_state != 10):\n",
    "                        P[current_state, next_state] = 0.05\n",
    "                    else:\n",
    "                        P[current_state,current_state] += 0.05\n",
    "                else:\n",
    "                    P[current_state, current_state] += 0.05\n",
    "\n",
    "    # all actions in the cell with the lightning bolt and treasure chest will keep the agent in the current cell. \n",
    "    # this means P[3,3] = 1, P[5,5] = 1\n",
    "    P[3] = np.array([0, 0, 0 , 1, 0, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0 ,0])\n",
    "    P[5] = np.array([0, 0, 0 , 0, 0, 1, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0 ,0])\n",
    "    \n",
    "    return P\n",
    "\n",
    "P = computeTransitionFunction(grid_actions) # compute the transition function given grid_actions above.\n",
    "\n",
    "# for i in range(P.shape[0]):\n",
    "#     print('row', i)\n",
    "#     print(P[i])\n",
    "markovianCheck(P) # confirm that P is indeed a row stochaistic matrix.\n",
    "\n",
    "#Analytical Solution:\n",
    "gamma = 0.95\n",
    "R_pi = np.zeros((16, 1))\n",
    "R_pi[3] = 1 # reward at cell 3\n",
    "R_pi[5] = -1 # reward at cell 5\n",
    "\n",
    "v_pi_analytical = np.linalg.inv(np.eye(16) - (gamma * P)) @ R_pi\n",
    "for i in range(16):\n",
    "    print('value function at state', i, v_pi_analytical[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291c3fa",
   "metadata": {},
   "source": [
    "Implement and run the iterative solution for approximate policy evaluation with 0 initialization for the value function.\n",
    "\n",
    "We want the final L-infinity norm || VT - V_pi || <= 0.01 without looking at the results of previous part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "69a66bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations we should use is 149\n",
      "value function at state 0 is [11.2244465]\n",
      "value function at state 1 is [12.05482402]\n",
      "value function at state 2 is [14.73557453]\n",
      "value function at state 3 is [19.99040937]\n",
      "value function at state 4 is [8.93103601]\n",
      "value function at state 5 is [-19.99040937]\n",
      "value function at state 6 is [15.49710191]\n",
      "value function at state 7 is [18.58949541]\n",
      "value function at state 8 is [8.27239676]\n",
      "value function at state 9 is [6.34025473]\n",
      "value function at state 10 is [15.83211713]\n",
      "value function at state 11 is [17.44427117]\n",
      "value function at state 12 is [7.72220448]\n",
      "value function at state 13 is [6.50428185]\n",
      "value function at state 14 is [14.92751377]\n",
      "value function at state 15 is [16.34787858]\n"
     ]
    }
   ],
   "source": [
    "v0 = np.zeros((16,1))\n",
    "\n",
    "# Number of iterations is T >= log|| v0 - v_pi|| / log (1 / gamma)\n",
    "# We know that if reward is bounded, so is any value function for a policy. That bound is (1 / 1 - gamma).\n",
    "# In this case, we can see that 1 / 1 - 0.95 = 20. Therefore, we need to compute L infinity norm of \n",
    "# (0 - 20).\n",
    "\n",
    "bound = 1 / (1 - gamma)\n",
    "v_pi = np.ones((16, 1)) * bound\n",
    "\n",
    "numerator = np.log(np.linalg.norm((v0 - v_pi), ord=np.inf)/ 0.01)\n",
    "denominator = np.log(1/gamma)\n",
    "T = int(numerator / denominator)\n",
    "print(\"Number of iterations we should use is\", T + 1)\n",
    "#Use same P transition function, gamma, and R, reward function from above.\n",
    "Vt_plus_1 = R_pi + gamma * P @ v0\n",
    "\n",
    "for i in range(T + 1):\n",
    "    if i == 0:\n",
    "        Vt_plus_1 = R_pi + gamma * P @ v0\n",
    "        vt = Vt_plus_1\n",
    "    else:\n",
    "        Vt_plus_1 = R_pi + gamma * P @ vt\n",
    "        vt = Vt_plus_1\n",
    "\n",
    "\n",
    "for i in range(Vt_plus_1.shape[0]):\n",
    "    print('value function at state', i, 'is', vt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06867a",
   "metadata": {},
   "source": [
    "Plot the sequence of errors in the value function from the approximate policy evaluation wrt the iterations. Specifically, plot ||vt - v_pi||inf against t, where Vt is the value function and v_pi is the value function computed from the analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3f3b9005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v_pi is the analytical soution.\n",
    "error = []\n",
    "v_pi_analytical = np.linalg.inv(np.eye(16) - (gamma * P)) @ R_pi\n",
    "for i in range(T + 1):\n",
    "    if i == 0:\n",
    "        Vt_plus_1 = R_pi + gamma * P @ v0\n",
    "        vt = Vt_plus_1\n",
    "        \n",
    "        temp_error_L_inf = np.linalg.norm((vt - v_pi_analytical), ord=np.inf)\n",
    "        error.append(temp_error_L_inf)\n",
    "    else:\n",
    "        Vt_plus_1 = R_pi + gamma * P @ vt\n",
    "        vt = Vt_plus_1\n",
    "        temp_error_L_inf = np.linalg.norm((vt - v_pi_analytical), ord=np.inf)\n",
    "        error.append(temp_error_L_inf)\n",
    "        \n",
    "plt.plot(error)\n",
    "plt.title('Error between policy and true solution per iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8f993",
   "metadata": {},
   "source": [
    "Implement and run a value iteration algorithm to compute an optimal policy in this MDP. Initialize the value function at zero. Pick the number of iterations in a way to ensure 0.01 accuracy in the final computed value function. Demonstrate the learned policy and report the value function at all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d94a4d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final learned policy (state 0 is at the top left, state 15 is at bottom right. States increase from right to left, top to bottom):\n",
      "treasure chest is at top right\n",
      "[['r' 'r' 'r' 'u']\n",
      " ['u' 'u' 'r' 'u']\n",
      " ['d' 'r' 'r' 'u']\n",
      " ['r' 'r' 'r' 'u']]\n",
      "final cummulative expected rewards / value function at each state:\n",
      "\n",
      "value function output at state 0 is [14.36065177]\n",
      "value function output at state 1 is [15.40036559]\n",
      "value function output at state 2 is [18.49741051]\n",
      "value function output at state 3 is [19.99040937]\n",
      "value function output at state 4 is [11.81068687]\n",
      "value function output at state 5 is [-19.99040937]\n",
      "value function output at state 6 is [15.69393381]\n",
      "value function output at state 7 is [18.6003618]\n",
      "value function output at state 8 is [12.69495447]\n",
      "value function output at state 9 is [13.42441037]\n",
      "value function output at state 10 is [16.20878528]\n",
      "value function output at state 11 is [17.4562319]\n",
      "value function output at state 12 is [13.49059156]\n",
      "value function output at state 13 is [14.37319307]\n",
      "value function output at state 14 is [15.37135073]\n",
      "value function output at state 15 is [16.38232712]\n"
     ]
    }
   ],
   "source": [
    "# pseudocode for optimal policy in value iteration:\n",
    "# initialize all value function outputs to be 0 (i.e, [v(0)... v(15)] = 0)\n",
    "# for t in range(T+1):\n",
    "#     for every state:\n",
    "#        compute value(s) = reward(s) + gamma * max(sum_over_all_s'(p(s'|s, a) * v(s'))).\n",
    "#NOTE: assume we are starting from cell 12 and action is UP. \n",
    "#Then, p(s'|s,a) = p(13) * v(13) + p(8) * v(8) + p(12) * v(12)\n",
    "#.               = 0.05 * 0 + 0.85 * 0 + 0.1 * 0\n",
    "# If we start from cell 12 and action is RIGHT:\n",
    "# Then, p(s'|s, a) = p(13) * v(13) + p(8) * v(8) + p(12) * v(12) again, but, the probabilities are different because\n",
    "# we have a different action?\n",
    "# In this case, p(13) = 0.85, p(8) = 0.05, p(12) = 0.1\n",
    "# We go through each action in each state, and the resulting v(s) = reward(s) + gamma * max(p(s'|s, a) * v(s')).\n",
    "# For example, if in s(12), we find that action UP gives us the best sum(p(13) * v(13) + p(8) * v(8) + p(12) * v(12)),\n",
    "# then, we choose that in computing v(12) = reward(s) + gamma * p(s'|s,a ) * v(s')\n",
    "# Now, how do we get from that to the actual POLICY? \n",
    "# After a certain number of iterations, we will have optimal value functions for every state. \n",
    "# To get policy for each state, get the value function associated with each possible action. Pick the action that \n",
    "# maximizes that value function. \n",
    "v_update = np.zeros((16,1))\n",
    "v_init = np.zeros((16,1))\n",
    "actions = ['u', 'd', 'r', 'l']\n",
    "action_dict = {0: 'u', 1: 'd', 2: 'r', 3: 'l'}\n",
    "selected_actions = np.zeros((16,1)) # store policy\n",
    "\n",
    "for t in range(0,T+1):\n",
    "        if t == 0:\n",
    "            for state, v_s in enumerate(v_init): # iterate through states\n",
    "                discounted_rewards = np.zeros((4,1)) # placeholder for disounted rewards\n",
    "                \n",
    "                for a, action in enumerate(actions):  # iterate through actions\n",
    "                    #print('action is', action)\n",
    "                    #need logic to convert row state to row and col. \n",
    "                    # if 15, this means 3,3. if 0, this means 0, 0. if 12, this means \n",
    "                    \n",
    "                    grid_actions[state // 4, state % 4] = action # compute the correct transition matrix for this action\n",
    "                    #print('grid_actions is', grid_actions)\n",
    "                    transition_matrix = np.zeros((16,16))\n",
    "                    transition_matrix = computeTransitionFunction(grid_actions)\n",
    "                    \n",
    "                    markovianCheck(transition_matrix) # ensure that properties of a markovian matrix hold\n",
    "                    # after computing the transition function\n",
    "                    \n",
    "                    for n, prob in enumerate(transition_matrix[state, :]):\n",
    "                        if (prob != 0):\n",
    "                            discounted_rewards[a] += prob * v_init[n] # compute p(s'|s,a) * v(s')\n",
    "                    \n",
    "                v_update[state] = R_pi[state] + gamma * np.max(discounted_rewards)\n",
    "                selected_actions[state] = np.argmax(discounted_rewards) # store policy for this iteration. \n",
    "            #print('new v', v_update)\n",
    "        if t > 0:\n",
    "            for state, v_s in enumerate(v_update): \n",
    "                discounted_rewards = np.zeros((4,1)) # placeholder for disounted rewards\n",
    "                \n",
    "                for a, action in enumerate(actions):\n",
    "                    grid_actions[state // 4, state % 4] = action # compute the correct transition matrix for this action\n",
    "                    transition_matrix = np.zeros((16,16))\n",
    "                    transition_matrix = computeTransitionFunction(grid_actions)\n",
    "                    markovianCheck(transition_matrix)\n",
    "\n",
    "                    \n",
    "                    for n, prob in enumerate(transition_matrix[state, :]):\n",
    "                        if (prob != 0):\n",
    "                            discounted_rewards[a] += prob * v_update[n]\n",
    "                            #print('just computed ', prob, 'times', v_update[n])\n",
    "                v_update[state] = R_pi[state] + gamma * np.max(discounted_rewards)\n",
    "                selected_actions[state] = np.argmax(discounted_rewards)   \n",
    "                \n",
    "                \n",
    "### Done computing optimal policy - displaying results below ###\n",
    "\n",
    "selected_actions = selected_actions.reshape(4,4)\n",
    "action_visualization = np.array([['x','x','x','x'], ['x','x','x','x'], ['x','x','x','x'], ['x','x','x','x']])\n",
    "print('final learned policy (state 0 is at the top left, state 15 is at bottom right. States increase from right to left, top to bottom):')\n",
    "print('treasure chest is at top right')\n",
    "for row in range(selected_actions.shape[0]):\n",
    "    for col in range(selected_actions.shape[1]):\n",
    "        action_visualization[row,col] = action_dict[selected_actions[row,col]]\n",
    "print(action_visualization)\n",
    "print('final cummulative expected rewards / value function at each state:\\n')\n",
    "for n, i in enumerate(v_update):\n",
    "    print('value function output at state', n, 'is', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5310863",
   "metadata": {},
   "source": [
    "Implement and run a policy iteration algorithm to compute an optimal policy in this MDP. Initialize the policy randomly by using a uniform distribution over all actions. Pick the number of iterations in a way to ensure 0.01 accuracy in the final computed value function. Demonstrate the learned policy and report the value function at all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d3197064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly initialized policy:\n",
      " [['d' 'd' 'u' 'u']\n",
      " ['u' 'u' 'd' 'l']\n",
      " ['u' 'l' 'u' 'u']\n",
      " ['d' 'd' 'l' 'u']]\n",
      "final policy:\n",
      "[['r' 'r' 'r' 'u']\n",
      " ['u' 'u' 'r' 'u']\n",
      " ['d' 'r' 'r' 'u']\n",
      " ['r' 'r' 'r' 'u']]\n",
      "final value function at each state\n",
      "[[ 14.36891147]\n",
      " [ 15.40870358]\n",
      " [ 18.50683203]\n",
      " [ 20.        ]\n",
      " [ 11.81761561]\n",
      " [-20.        ]\n",
      " [ 15.7019808 ]\n",
      " [ 18.60936494]\n",
      " [ 12.70279695]\n",
      " [ 13.43185352]\n",
      " [ 16.21717086]\n",
      " [ 17.46475832]\n",
      " [ 13.49853165]\n",
      " [ 14.38116197]\n",
      " [ 15.3793975 ]\n",
      " [ 16.39040191]]\n"
     ]
    }
   ],
   "source": [
    "# pseudocode for optimal policy in policy iteration:\n",
    "# initialize policy uniformly (i.e, 1/4 chance state will have up, right, down, left)\n",
    "# for t in range(T+1):\n",
    "#      evaluate policy for every state (can use v_pi_analytical = np.linalg.inv(np.eye(16) - (gamma * P)) @ R_pi)\n",
    "#      improve policy - find argmax of immediate reward at that state + gamma * E[v_pi(s')]\n",
    "#      v_pi(s') is the probability of transitioning to s' * the value at that state. For example, if the\n",
    "#      probability of going to s' is 0.05, we multiply 0.05 * v_pi(s'). We need to do this for every state\n",
    "#      and action (I think). \n",
    "\n",
    "random_policy = np.random.randint(4, size=(4,4))\n",
    "for row in range(random_policy.shape[0]):\n",
    "    for col in range(random_policy.shape[1]):\n",
    "        grid_actions[row,col] = action_dict[int(random_policy[row,col])]\n",
    "\n",
    "print('randomly initialized policy:\\n',grid_actions)\n",
    "\n",
    "temp_grid_actions = grid_actions # create a copy - we need one to create the transition matrix but also do not want\n",
    "# to alter the original policy so we can update the original poicy correctly! \n",
    "\n",
    "# Reuse reward function from above. Re-instantiating those variables here for clarity.\n",
    "R_pi = np.zeros((16, 1))\n",
    "R_pi[3] = 1 # reward at cell 3\n",
    "R_pi[5] = -1 # reward at cell 5\n",
    "\n",
    "value_function_update = np.zeros((16,1)) # init 16 by 1 vector to hold updated  \n",
    "selected_actions = np.zeros((16,1)) # store policy\n",
    "\n",
    "for t in range(1,T+1):\n",
    "    #evaluate policy for each state - use analytical solution:\n",
    "    P = computeTransitionFunction(grid_actions) # compute the transition function given current policy stored in\n",
    "    # grid_actions.\n",
    "    markovianCheck(P) # confirm that P is in fact a valid markovian, row stochaistic matrix \n",
    "    # we will see \"error\" printed below if there is a row that doesn't follow the markovian matrix properties\n",
    "    #Analytical Solution:\n",
    "    gamma = 0.95\n",
    "    v_pi_analytical = np.linalg.inv(np.eye(16) - (gamma * P)) @ R_pi\n",
    "    #print(v_pi_analytical)\n",
    "    \n",
    "    # update policy based on analytical solution.\n",
    "    for state, v_s in enumerate(v_pi_analytical):\n",
    "        \n",
    "        discounted_rewards = np.zeros((4,1)) # placeholder for disounted rewards\n",
    "\n",
    "        for a, action in enumerate(actions):\n",
    "            temp_grid_actions[state // 4, state % 4] = action # update the action for the state we are computing \n",
    "            # p(s' | s, a) for\n",
    "            # update transition matrix based on this update to grid_actions\n",
    "            transition_matrix = computeTransitionFunction(grid_actions)\n",
    "            markovianCheck(transition_matrix) # confirm we have computed a valid transition function/operator\n",
    "            \n",
    "            for n, prob in enumerate(transition_matrix[state, :]):\n",
    "                if (prob != 0):\n",
    "                    discounted_rewards[a] += prob * v_pi_analytical[n]\n",
    "        \n",
    "        temp_array = np.array([R_pi[state], R_pi[state], R_pi[state], R_pi[state]]) + gamma * discounted_rewards\n",
    "        selected_actions[state] = np.argmax(temp_array)\n",
    "\n",
    "        \n",
    "        #update policy:\n",
    "        grid_actions[state // 4, state % 4] = action_dict[int(selected_actions[state])]\n",
    "        \n",
    "print('final policy:')\n",
    "print(grid_actions)\n",
    "print('final value function at each state')\n",
    "print(v_pi_analytical)\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e97d8f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "16.1245154965971\n",
      "30.0\n",
      "14\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([2, 3])\n",
    "x2 = np.array([4,2])\n",
    "\n",
    "x1_Linf = np.linalg.norm(x1, ord=np.inf)\n",
    "print(x1_Linf)\n",
    "\n",
    "x1_L2 =np.linalg.norm(x1 , ord=2)\n",
    "x2_L2 = np.linalg.norm(x2, ord=2)\n",
    "x1x2_L2 = x1_L2 * x2_L2\n",
    "print(x1x2_L2)\n",
    "\n",
    "x1_L1 = np.linalg.norm(x1, ord=1)\n",
    "x2_L1 = np.linalg.norm(x2, ord=1)\n",
    "print(np.sqrt(x1_L1**2 * x2_L1**2))\n",
    "print(np.dot(x1, x2))\n",
    "\n",
    "print(1 % 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ece637]",
   "language": "python",
   "name": "conda-env-ece637-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
